"""
COMP 532 - REINFORCEMENT LEARNING ASSIGNMENT 1
SUBMITTED BY : GROUP 1
USMAN SHOUKAT   					     STUDENT ID: 201537600
JONE CHONG							     STUDENT ID: 201533109
SAHIB BIR SINGH BHATIA 			 	     STUDENT ID: 201547831
PARTH KHANDELWAL					     STUDENT ID: 201549274
"""
import numpy as np
import matplotlib.pyplot as plt
from numpy.core.fromnumeric import argmax
from tqdm import tqdm



class Environment:
    '''
    This  class initialises all the parameters of the environment
    '''
    def __init__(self, arms = 10):
        self.mu =  np.random.normal(0,1,arms)
        self.n_arms = arms
        self.n_counter = np.zeros(arms)
        self.sumOfEachN = np.zeros(arms)
        self.qValues = np.zeros(arms)

class Agent():
    def __init__(self, eps, environment, plays = 1000):
        self.environment = environment
        self.plays = plays
        self.eps = eps
        self.cumulativeReward = np.zeros(plays)
        self.optimalAction = np.zeros(plays)


    def pull(self):
        '''
        This method pulls the arm following epsilon greedy policy with given epsilon value
        and returns the arm and reward for pulling that arm
        Parameters:
            self
        Return:
            Reward
            Arm being pulled

        '''
        #Choosing the random value to decide whether to explore or exploit while comparing with epsilon
        #value. 
        prob = np.random.rand()
        if prob < self.eps: #If chosen random value is less than epsilon then explore
            a = np.random.choice(self.environment.n_arms)
        elif prob > self.eps: #And if chosen random value is less than epsilon then exploit
            a = argmax(self.environment.qValues)
        reward = np.random.normal(self.environment.mu[a] , 1, 1)

        #Updating the mean of arm being pulled
        self.environment.n_counter[a] += 1
        self.environment.sumOfEachN[a] += reward
        self.environment.qValues[a] = self.environment.sumOfEachN[a] / self.environment.n_counter[a] 
        return reward, a
        
    def run(self):
        '''
        This method pulls the arm for given number of episodes, saves the cumulative reward and
        at the end of all episodes, reinitialise the environment
        
        '''
        sum = 0
        for i in range(self.plays):
            reward, a = self.pull()
            sum += reward
            self.cumulativeReward[i] += (sum/(i+1))
            if a == argmax(self.environment.mu): #checking if the optimal action was taken
                self.optimalAction[i] += 1
        self.reinitialisation()

    def reinitialisation(self):
        self.environment = Environment()
        


env = Environment()
#initialisation of agents
greedy = Agent(0, env)
eGreedy_01 = Agent(0.1, env)
eGreedy_001 = Agent(0.01, env)
iterations = 2000
#Running the experiment for 2000 iterations
for i in tqdm(range(iterations)):
    greedy.run()
    eGreedy_01.run()
    eGreedy_001.run()
   
#Averaging the reward over number of iterations
greedy.cumulativeReward /= iterations
eGreedy_01.cumulativeReward /= iterations
eGreedy_001.cumulativeReward /= iterations

#Calcuting the percentage optimal action
greedy.optimalAction *= (100/iterations)
eGreedy_01.optimalAction *= (100/iterations)
eGreedy_001.optimalAction *= (100/iterations)


#plotting of graphs
plt.subplot(2,1,1)
plt.plot(greedy.cumulativeReward, color = "r", label = "Greedy")
plt.plot(eGreedy_01.cumulativeReward, color = "b", label = "ε-Greedy with ε =0.1")
plt.plot(eGreedy_001.cumulativeReward, color = "g", label = "ε-Greedy with ε =0.01")
plt.ylim(0,1.5)
plt.ylabel("Average Reward")

plt.subplot(2,1,2)
plt.plot(greedy.optimalAction, color = "r", label = "Greedy")
plt.plot(eGreedy_01.optimalAction, color = "b", label = "ε-Greedy with ε =0.1")
plt.plot(eGreedy_001.optimalAction, color = "g", label = "ε-Greedy with ε =0.01")
plt.ylim(0,100)
plt.xlabel("Plays")
plt.ylabel("Optimal Action %")
plt.legend()
plt.show()